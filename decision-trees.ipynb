{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Brian McCollum, Estefan Gonzales, Cyrus McCormick\n## CS429 Project1: Random Forests","metadata":{}},{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nfrom enum import Enum\nfrom sklearn.model_selection import train_test_split\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-02-05T06:13:57.788738Z","iopub.execute_input":"2023-02-05T06:13:57.789159Z","iopub.status.idle":"2023-02-05T06:13:58.284299Z","shell.execute_reply.started":"2023-02-05T06:13:57.789125Z","shell.execute_reply":"2023-02-05T06:13:58.282905Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sample_df = pd.read_csv(\"/kaggle/input/contest-data/agaricus-lepiota - sample_solution.csv\")\ntest_df = pd.read_csv(\"/kaggle/input/contest-data/agaricus-lepiota - testing.csv\")\ntrain_df = pd.read_csv(\"/kaggle/input/contest-data/agaricus-lepiota - training.csv\")\n# split the dataframe into training and validation sets\ntrain_df, validation_df = train_test_split(train_df, test_size=0.2)","metadata":{"execution":{"iopub.status.busy":"2023-02-05T06:37:46.289553Z","iopub.execute_input":"2023-02-05T06:37:46.290385Z","iopub.status.idle":"2023-02-05T06:37:46.347309Z","shell.execute_reply.started":"2023-02-05T06:37:46.290330Z","shell.execute_reply":"2023-02-05T06:37:46.345766Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Node class","metadata":{}},{"cell_type":"code","source":"class AttrNode:\n    def __init__(self, name: str, info_gain: float, is_leaf=False):\n        self.name = name\n        self.info_gain = info_gain\n        self.children = {}\n        self.is_leaf = is_leaf\n\n    def __str__(self) -> str:\n        return f\"attribute=[name:{self.name}, info_gain:{self.info_gain}], is_leaf={self.is_leaf}\"","metadata":{"execution":{"iopub.status.busy":"2023-02-05T06:10:40.384361Z","iopub.execute_input":"2023-02-05T06:10:40.384709Z","iopub.status.idle":"2023-02-05T06:10:40.391638Z","shell.execute_reply.started":"2023-02-05T06:10:40.384678Z","shell.execute_reply":"2023-02-05T06:10:40.390603Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Information Gain","metadata":{}},{"cell_type":"code","source":"class iGainType(Enum):\n    entropy = 0\n    gini = 1\n    misclass = 2","metadata":{"execution":{"iopub.status.busy":"2023-02-05T06:10:40.397679Z","iopub.execute_input":"2023-02-05T06:10:40.398226Z","iopub.status.idle":"2023-02-05T06:10:40.407581Z","shell.execute_reply.started":"2023-02-05T06:10:40.398191Z","shell.execute_reply":"2023-02-05T06:10:40.406167Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Gini Index\n$1 - \\sum_{i=1}^{m} p_i^2$","metadata":{}},{"cell_type":"code","source":"def gini_index(data_set, classifier):\n    n = len(data_set.index)\n    prob = 0\n    class_dict = {}\n    for datum in data_set.loc[:, classifier]:\n        if datum not in class_dict:\n            class_dict[datum] = 1\n        else:\n            class_dict[datum] += 1\n    for key in class_dict:\n        prob += ((class_dict[key] / n) ** 2)\n    return 1 - prob","metadata":{"execution":{"iopub.status.busy":"2023-02-05T06:10:40.419705Z","iopub.execute_input":"2023-02-05T06:10:40.420988Z","iopub.status.idle":"2023-02-05T06:10:40.427821Z","shell.execute_reply.started":"2023-02-05T06:10:40.420931Z","shell.execute_reply":"2023-02-05T06:10:40.426218Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Entropy\n$\\sum_{i=1}^{m} - p_i log_2 p_i$","metadata":{}},{"cell_type":"code","source":"def entropy(data_set, classifier):\n    n = len(data_set.index)\n    prob = 0\n    class_dict = {}\n    for datum in data_set.loc[:, classifier]:\n        if datum not in class_dict:\n            class_dict[datum] = 1\n        else:\n            class_dict[datum] += 1\n    for key in class_dict:\n        p = (class_dict[key] / n)\n        prob += (p *  np.log2(p))\n    \n    return -1 * prob","metadata":{"execution":{"iopub.status.busy":"2023-02-05T06:10:40.437528Z","iopub.execute_input":"2023-02-05T06:10:40.438007Z","iopub.status.idle":"2023-02-05T06:10:40.448515Z","shell.execute_reply.started":"2023-02-05T06:10:40.437930Z","shell.execute_reply":"2023-02-05T06:10:40.447408Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Misclassification error\n\n$1 - \\max\\limits_k p_k$","metadata":{}},{"cell_type":"code","source":"def misclassification_error(data_set, classifier):\n    n = len(data_set.index)\n    probs = []\n    class_dict = {}\n    for datum in data_set.loc[:, classifier]:\n        if datum not in class_dict:\n            class_dict[datum] = 1\n        else:\n            class_dict[datum] += 1\n    for key in class_dict:\n        probs.append((class_dict[key] / n))\n        \n    return 1 - max(probs)","metadata":{"execution":{"iopub.status.busy":"2023-02-05T06:10:40.462877Z","iopub.execute_input":"2023-02-05T06:10:40.463300Z","iopub.status.idle":"2023-02-05T06:10:40.471273Z","shell.execute_reply.started":"2023-02-05T06:10:40.463267Z","shell.execute_reply":"2023-02-05T06:10:40.470226Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Tree creation","metadata":{}},{"cell_type":"markdown","source":"#### Defining helper functions for tree creation","metadata":{}},{"cell_type":"code","source":"def split(data_set, attr, classifier, criteria: iGainType = iGainType.entropy):\n    n = len(data_set.index)\n    attr_dict = {}\n    split_value = 0\n    for datum in data_set.loc[:, attr]:\n        if datum not in attr_dict:\n            attr_dict[datum] = 1\n        else:\n            attr_dict[datum] += 1\n    for attr_value in attr_dict:\n        # proportion\n        weight = attr_dict[attr_value] / n\n        # choose all examples with this attr value\n        subset = data_set.loc[data_set[attr] == attr_value]\n        \n        # impurity \n        attr_v = 0\n        if criteria == iGainType.gini:\n            attr_v = gini_index(subset, classifier)  \n        elif criteria == iGainType.entropy:\n            attr_v = entropy(subset, classifier)  \n        elif criteria == iGainType.misclass:\n            attr_v = misclassification_error(subset, classifier)  \n            # handle here w weight\n            \n        # info gain\n        split_value += (weight * attr_v)\n\n    return split_value\n\ndef split_values(data_set, classifier, attributes, criteria: iGainType = iGainType.entropy):\n    split_vals = {}\n    for col in attributes:\n        if col == classifier:\n            continue\n        \n        split_vals[col] = split(\n            data_set, col, classifier, criteria)\n \n    return split_vals\n\ndef majority_classification(data_set, classifier):\n    classification_count = {}\n    for datum in data_set.loc[:, classifier]:\n        if datum not in classification_count:\n            classification_count[datum] = 1\n        else:\n            classification_count[datum] += 1\n    majority = max(classification_count.values())\n    for key in classification_count.keys():\n        if classification_count[key] == majority:\n            return key\n    return -1\n\ndef best_attr(attr_split_values):\n    argmin = min(attr_split_values.values())\n    for k in attr_split_values.keys():\n        if attr_split_values[k] == argmin:\n            return k\n    print('none found')\n    return ''\n\ndef homogeneous(data_set, classifier):\n    class_set = {val for val in data_set.loc[:, classifier]}\n    return len(class_set) == 1\n\ndef classify(root: AttrNode, example: {}):\n    while not root.is_leaf:\n        ex_value = example[root.name]\n        root = root.children[ex_value]\n    return root.name","metadata":{"_kg_hide-output":true,"execution":{"iopub.status.busy":"2023-02-05T06:31:40.507119Z","iopub.execute_input":"2023-02-05T06:31:40.507502Z","iopub.status.idle":"2023-02-05T06:31:40.525585Z","shell.execute_reply.started":"2023-02-05T06:31:40.507470Z","shell.execute_reply":"2023-02-05T06:31:40.524406Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def create_decision_tree(data_set, classifier, attributes, examples, criteria: iGainType = iGainType.entropy):\n    if homogeneous(data_set, classifier) or len(attributes) == 1:\n        cls = majority_classification(data_set, classifier)\n        return AttrNode(cls, 0, True)\n\n    attr_split_values = split_values(\n        data_set, classifier, attributes, criteria)\n    best_classifier = best_attr(attr_split_values)\n    values = {val for val in examples.loc[:, best_classifier]}\n    root = AttrNode(best_classifier, attr_split_values[best_classifier])\n\n    for value in values:\n        value_subset = data_set.loc[data_set[best_classifier] == value]\n        if len(value_subset.index) == 0:\n            # no data records in subset, classification node\n            # w/ value set to most common class at root node\n            maj_class = majority_classification(data_set, classifier)\n            child = AttrNode(maj_class, 0, True)\n            root.children[value] = child\n        else:\n            # sub trees for remaining attributes\n            root.children[value] = create_decision_tree(value_subset,\n                                                       classifier,\n                                                       [atr for atr in attributes if atr !=\n                                                           best_classifier],\n                                                       examples)\n    return root","metadata":{"execution":{"iopub.status.busy":"2023-02-05T06:10:40.514717Z","iopub.execute_input":"2023-02-05T06:10:40.516527Z","iopub.status.idle":"2023-02-05T06:10:40.528605Z","shell.execute_reply.started":"2023-02-05T06:10:40.516468Z","shell.execute_reply":"2023-02-05T06:10:40.527304Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Train the model","metadata":{}},{"cell_type":"code","source":"training_data = train_df.loc[:, train_df.columns != 'id']\ngini_root = create_decision_tree(training_data, 'class', training_data.columns, training_data, iGainType.gini)\nentropy_root = create_decision_tree(training_data, 'class', training_data.columns, training_data, iGainType.entropy)\nmisclass_root = create_decision_tree(training_data, 'class', training_data.columns, training_data, iGainType.misclass)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Predicition","metadata":{}},{"cell_type":"code","source":"validation_dict = validation_df.to_dict('records')\ntest_dict = test_df.to_dict('records')\n\ntotal_correct = 0\ntotal = len(validation_dict)\n\nfor prediction in validation_dict:\n    actual_class = classify(misclass_root, prediction)\n    expected_class = prediction['class']\n    if actual_class == expected_class:\n        total_correct += 1","metadata":{"execution":{"iopub.status.busy":"2023-02-05T06:41:13.672309Z","iopub.execute_input":"2023-02-05T06:41:13.673099Z","iopub.status.idle":"2023-02-05T06:41:13.750435Z","shell.execute_reply.started":"2023-02-05T06:41:13.673056Z","shell.execute_reply":"2023-02-05T06:41:13.749093Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(gini_root)\nprint(entropy_root)\nprint(misclass_root)\n\naccuracy = total_correct / total\nprint(f'\\ncorrect={total_correct}, total={total}, accuracy={accuracy}')","metadata":{"execution":{"iopub.status.busy":"2023-02-05T06:41:32.854523Z","iopub.execute_input":"2023-02-05T06:41:32.855030Z","iopub.status.idle":"2023-02-05T06:41:32.862660Z","shell.execute_reply.started":"2023-02-05T06:41:32.854988Z","shell.execute_reply":"2023-02-05T06:41:32.861360Z"},"trusted":true},"execution_count":null,"outputs":[]}]}